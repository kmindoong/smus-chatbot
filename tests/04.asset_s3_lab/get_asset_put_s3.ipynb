{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Python SDK(boto3) DataZone Clientì˜ Search ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ Asset ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "\n",
    "- ì•„ë˜ **í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ IAM ì •ë³´**ëŠ” ì‹¤ì œ í˜¸ì¶œí•  ì£¼ì²´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì‹¤í–‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ IAM Identity ì •ë³´:\n",
      "Account ID : 533616270150\n",
      "UserId     : AIDAXYPQCDNDD6RMCGL25\n",
      "Arn        : arn:aws:iam::533616270150:user/mj.kwon@sk.com\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "\n",
    "print(\"âœ… í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ IAM Identity ì •ë³´:\")\n",
    "print(f\"Account ID : {identity['Account']}\")\n",
    "print(f\"UserId     : {identity['UserId']}\")\n",
    "print(f\"Arn        : {identity['Arn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset ë©”íƒ€ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ **SMUS Notebook** ë‚´ì—ì„œ ì‹¤í–‰í•  ê²½ìš°ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ì¶œë ¥ë¨.\n",
    "\n",
    "âœ… **í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ IAM Identity ì •ë³´**\n",
    "\n",
    "| í•­ëª© | ê°’ |\n",
    "|------|----|\n",
    "| Account ID | `533616270150` |\n",
    "| UserId | `AROAXYPQCDNDMUFTA7JFO:SageMaker` |\n",
    "| Arn | `arn:aws:sts::533616270150:assumed-role/datazone_usr_role_bknisgxzopjuhk_4oa8qs5krhk8ig/SageMaker` |\n",
    "\n",
    "---\n",
    "\n",
    "### SMUS Notebookì—ì„œ ì‹¤í–‰í•  ê²½ìš° ì•„ë˜ ì •ì±…ì„ **`datazone_usr_role_bknisgxzopjuhk_4oa8qs5krhk8ig`** ì—­í• ì— ì¶”ê°€í•´ì•¼ í•¨:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"AllowDataZoneReadAccess\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"datazone:Search\",\n",
    "        \"datazone:GetAsset\",\n",
    "        \"datazone:GetDomain\",\n",
    "        \"datazone:ListProjects\",\n",
    "        \"datazone:GetProject\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:datazone:ap-northeast-2:533616270150:domain/dzd-cjvglgj4d43fmg\",\n",
    "        \"arn:aws:datazone:ap-northeast-2:533616270150:domain/dzd-cjvglgj4d43fmg/*\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMUS Domain ë‚´ ì „ì²´ Assetì„ ì¡°íšŒ\n",
    "- AWS SDK DataZone Clientì˜ Searchë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©°, ì•„ë˜ ìì„¸í•œ ì •ë³´ í™•ì¸ ê°€ëŠ¥\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/datazone/client/search.html\n",
    "- Searchë©”ì„œë“œëŠ” searchScope ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒê°€ ê°€ëŠ¥í•˜ë©°, ì•„ë˜ì™€ ê°™ì´ 4ê°€ì§€ë¥¼ ì§€ì›\n",
    "- searchScope: \"ASSET\" || \"GLOSSARY\" || \"GLOSSARY_TERM\" || \"DATA_PRODUCT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì•„ë˜ì˜ ì½”ë“œëŠ” ì „ì²´ Asset ì¤‘ GlueTableAssetType + listing_status == \"ACTIVE\" (PUBLISHED)ë§Œ ì°¾ì•„ì„œ í•´ë‹¹ Asset ë³„ ë©”íƒ€ë°ì´í„°ë¥¼ \"[Assetëª…]_[AssetID].json\"í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ì˜ˆì‹œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì´ˆê¸°ì ì¬, ë³€ê²½ë¶„ ë°˜ì˜ ì½”ë“œ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ˆê¸°ì ì¬ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total 50 GlueTableAssetType assets found by search. Starting detail fetch...\n",
      "\n",
      "âœ… Final Count: 2 PUBLISHED assets ready for initial load.\n",
      "\n",
      "--- Step 4: Saving ALL JSON files and creating initial snapshot ---\n",
      "âœ… Saved Asset: asset_result_data/glue_assets_kb_sync_final_test/cloudtrail_logs_s3_trail_events.json\n",
      "âœ… Saved Asset: asset_result_data/glue_assets_kb_sync_final_test/glue_db_cwkk5edf43ynmg_store_details.json\n",
      "\n",
      "ğŸ‰ Initial Full Load Complete. (2 assets saved)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "from datetime import datetime, timezone \n",
    "\n",
    "# --- í™˜ê²½ ì„¤ì • ---\n",
    "region = \"ap-northeast-2\"\n",
    "dz = boto3.client(\"datazone\", region_name=region)\n",
    "domain_id = \"dzd-3ojk7mnm02q5lk\"\n",
    "MAX_THREADS = 50 \n",
    "\n",
    "# ğŸ’¡ KB Data Sourceê°€ ë°”ë¼ë³¼ ìµœì¢… ë™ê¸°í™” í´ë” (ì „ì²´/ë¸íƒ€ ê³µìš©)\n",
    "KB_S3_SYNC_DIR = \"asset_result_data/glue_assets_kb_sync_final\" \n",
    "os.makedirs(KB_S3_SYNC_DIR, exist_ok=True)\n",
    "PUBLISHED_ASSET_SNAPSHOT_FILE = \"published_asset_update_snapshot_initial.json\" # ìŠ¤ëƒ…ìƒ· íŒŒì¼ëª…\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ğŸ’¡ í—¬í¼ í•¨ìˆ˜ ë° Step 3 í•¨ìˆ˜ ì •ì˜\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def format_time_to_iso(dt_object):\n",
    "    \"\"\"datetime ê°ì²´ë¥¼ KB ê´€ë¦¬ì— í•„ìš”í•œ UTC ISO ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if dt_object and isinstance(dt_object, datetime):\n",
    "        return dt_object.astimezone(timezone.utc).isoformat().replace('+00:00', 'Z')\n",
    "    if isinstance(dt_object, str):\n",
    "        return dt_object.replace('+00:00', 'Z') if '+00:00' in dt_object else dt_object\n",
    "    return None\n",
    "\n",
    "def fetch_and_filter_asset(asset_info):\n",
    "    \"\"\"ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€ ìƒíƒœ í™•ì¸ ë° ê³ ìœ  í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    asset_id = asset_info[\"id\"]\n",
    "    \n",
    "    try:\n",
    "        asset_resp = dz.get_asset(domainIdentifier=domain_id, identifier=asset_id)\n",
    "        \n",
    "        # 1. GlueTableForm JSON ë¬¸ìì—´ ì¶”ì¶œ ë° íŒŒì‹±\n",
    "        glue_form_content = next((item['content'] for item in asset_resp.get('formsOutput', []) \n",
    "                                  if item['formName'] == 'GlueTableForm'), None)\n",
    "        \n",
    "        if not glue_form_content: return None\n",
    "        glue_form_data = json.loads(glue_form_content)\n",
    "\n",
    "        # 2. ê³ ìœ  í‚¤ ìƒì„±\n",
    "        db_name = glue_form_data.get(\"databaseIdentifier\") or glue_form_data.get(\"databaseName\")\n",
    "        table_name = glue_form_data.get(\"tableName\")\n",
    "        if not db_name or not table_name: return None\n",
    "        unique_key = f\"{db_name}_{table_name}\"\n",
    "        \n",
    "        # ğŸš€ [ìµœì¢… ìˆ˜ì •] 3. ìµœì¢… ì—…ë°ì´íŠ¸ ì‹œê°„ (Asset Revision ê¸°ë°˜) ì¶”ì¶œ\n",
    "        # Asset ë©”íƒ€ë°ì´í„°ê°€ ìˆ˜ì •ëœ ì‹œê°„: Boto3ëŠ” ì´ í•„ë“œë¥¼ 'createdAt'ìœ¼ë¡œ ë°˜í™˜\n",
    "        current_update_time_dt = asset_resp.get(\"createdAt\") \n",
    "        current_time_str = format_time_to_iso(current_update_time_dt) # ISO ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        \n",
    "        # 4. ê²Œì‹œ ìƒíƒœ í™•ì¸\n",
    "        listing_status = (asset_resp.get(\"listing\", {}) or {}).get(\"listingStatus\", \"UNKNOWN\")\n",
    "        \n",
    "        if listing_status == \"ACTIVE\":\n",
    "            asset_info.update({\n",
    "                \"unique_key\": unique_key, \n",
    "                # ğŸš€ [ìˆ˜ì • ì ìš©] ì¶”ì¶œí•œ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ì €ì¥\n",
    "                \"current_update_time\": current_time_str, # Asset Revision ì‹œê°„ ì €ì¥\n",
    "                \"metadata\": asset_resp\n",
    "            })\n",
    "            return asset_info\n",
    "            \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"âŒ Error fetching asset details for {asset_id}: {e}\") # ë””ë²„ê·¸ìš©\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ëª¨ë“  GlueTableAssetTypeì˜ ID ìˆ˜ì§‘ (ì „ì²´ ì¡°íšŒ)\n",
    "# ----------------------------------------------------------------------\n",
    "projects_resp = dz.list_projects(domainIdentifier=domain_id)\n",
    "projects = {p[\"id\"]: p[\"name\"] for p in projects_resp.get(\"items\", [])}\n",
    "all_glue_asset_ids = []\n",
    "\n",
    "for project_id, project_name in projects.items():\n",
    "    next_token = None\n",
    "    while True:\n",
    "        params = {\"domainIdentifier\": domain_id, \"owningProjectIdentifier\": project_id, \"searchScope\": \"ASSET\"}\n",
    "        if next_token: params[\"nextToken\"] = next_token\n",
    "\n",
    "        resp = dz.search(**params)\n",
    "        for item in resp.get(\"items\", []):\n",
    "            if item.get(\"assetItem\", {}).get(\"typeIdentifier\") == \"amazon.datazone.GlueTableAssetType\":\n",
    "                all_glue_asset_ids.append({\"id\": item.get(\"assetItem\", {}).get(\"identifier\"), \"name\": item.get(\"assetItem\", {}).get(\"name\")})\n",
    "        next_token = resp.get(\"nextToken\")\n",
    "        if not next_token: break\n",
    "\n",
    "print(f\"âœ… Total {len(all_glue_asset_ids)} GlueTableAssetType assets found by search. Starting detail fetch...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3ï¸âƒ£ get_asset í˜¸ì¶œ ë° ìµœì¢… í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰)\n",
    "# ----------------------------------------------------------------------\n",
    "published_assets = []\n",
    "MAX_THREADS = 50 \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    results = executor.map(fetch_and_filter_asset, all_glue_asset_ids)\n",
    "    for asset in results:\n",
    "        if asset:\n",
    "            published_assets.append(asset)\n",
    "\n",
    "print(f\"\\nâœ… Final Count: {len(published_assets)} PUBLISHED assets ready for initial load.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4ï¸âƒ£ JSON ì €ì¥ ë° ìŠ¤ëƒ…ìƒ· ìƒì„± (ì´ˆê¸° ì ì¬)\n",
    "# ----------------------------------------------------------------------\n",
    "current_update_snapshot = {}\n",
    "\n",
    "print(\"\\n--- Step 4: Saving ALL JSON files and creating initial snapshot ---\")\n",
    "\n",
    "for asset in published_assets:\n",
    "    unique_key = asset[\"unique_key\"]\n",
    "    output_path = os.path.join(KB_S3_SYNC_DIR, f\"{unique_key}.json\") \n",
    "    update_dt = asset[\"current_update_time\"] # Asset Revision ì‹œê°„\n",
    "\n",
    "    # 4-A. JSON íŒŒì¼ ì €ì¥\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\"unique_key\": unique_key, \"metadata\": asset[\"metadata\"], \"updatedAt\": update_dt}, \n",
    "            f, default=str, ensure_ascii=False, indent=2\n",
    "        )\n",
    "    \n",
    "    current_update_snapshot[unique_key] = update_dt\n",
    "    print(f\"âœ… Saved Asset: {output_path}\")\n",
    "\n",
    "# 4-C. ë‹¤ìŒ ì¼ ë°°ì¹˜ë¥¼ ìœ„í•´ ìŠ¤ëƒ…ìƒ· íŒŒì¼ ì €ì¥\n",
    "with open(PUBLISHED_ASSET_SNAPSHOT_FILE, \"w\") as f:\n",
    "    json.dump(current_update_snapshot, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ‰ Initial Full Load Complete. ({len(current_update_snapshot)} assets saved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë³€ê²½ë¶„ ë°˜ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 update records for delta comparison.\n",
      "\n",
      "--- Step 2: Collecting ALL Asset IDs for Delta Check ---\n",
      "âœ… Total 50 assets found for detailed check.\n",
      "\n",
      "âœ… Found 1 changes (CREATED/UPDATED) to synchronize.\n",
      "âœ… KB File CREATED: default_cloudtrail_logs_mjkwon_smus_test.json\n",
      "\n",
      "ğŸ‰ Daily Delta Asset Management Complete. (Saved 1 changes, Deleted 0 keys)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "from datetime import datetime, timedelta, timezone \n",
    "\n",
    "# --- í™˜ê²½ ì„¤ì • ---\n",
    "region = \"ap-northeast-2\"\n",
    "dz = boto3.client(\"datazone\", region_name=region)\n",
    "domain_id = \"dzd-3ojk7mnm02q5lk\"\n",
    "MAX_THREADS = 50 \n",
    "\n",
    "# ğŸ’¡ KB ë™ê¸°í™” ê²½ë¡œ ë° ìŠ¤ëƒ…ìƒ· íŒŒì¼ (ì´ˆê¸° ì ì¬ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•´ì•¼ í•¨)\n",
    "KB_S3_SYNC_DIR = \"asset_result_data/glue_assets_kb_sync_final\" \n",
    "PUBLISHED_ASSET_SNAPSHOT_FILE = \"published_asset_update_snapshot_initial.json\" \n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ğŸ’¡ í—¬í¼ í•¨ìˆ˜ ì •ì˜\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def time_str_to_datetime(time_str):\n",
    "    \"\"\"ISO 8601 ë¬¸ìì—´ì„ UTC datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not time_str or time_str == 'None': return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    if isinstance(time_str, datetime): dt = time_str\n",
    "    elif isinstance(time_str, str):\n",
    "        time_str = time_str.replace('Z', '+00:00') if time_str.endswith('Z') else time_str\n",
    "        try: dt = datetime.fromisoformat(time_str)\n",
    "        except ValueError: dt = datetime.fromisoformat(time_str).replace(tzinfo=timezone.utc)\n",
    "    else: return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def is_time_newer(current_time_str, previous_time_str):\n",
    "    \"\"\"í˜„ì¬ ì‹œê°„ì´ ì´ì „ ì €ì¥ ì‹œê°„ë³´ë‹¤ ìµœì‹ ì¸ì§€ ë¹„êµí•©ë‹ˆë‹¤.\"\"\"\n",
    "    current_dt = time_str_to_datetime(current_time_str)\n",
    "    previous_dt = time_str_to_datetime(previous_time_str)\n",
    "    return current_dt > previous_dt\n",
    "\n",
    "def format_time_to_iso(dt_object):\n",
    "    \"\"\"datetime ê°ì²´ë¥¼ KB ê´€ë¦¬ì— í•„ìš”í•œ UTC ISO ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if dt_object and isinstance(dt_object, datetime):\n",
    "        return dt_object.astimezone(timezone.utc).isoformat().replace('+00:00', 'Z')\n",
    "    if isinstance(dt_object, str):\n",
    "        return dt_object.replace('+00:00', 'Z') if '+00:00' in dt_object else dt_object\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_and_filter_asset(asset_info):\n",
    "    \"\"\"ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€ ìƒíƒœ í™•ì¸ ë° ê³ ìœ  í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    asset_id = asset_info[\"id\"]\n",
    "    try:\n",
    "        # Boto3 í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ì€ ì•ˆì •ì„±ì„ ìœ„í•´ ìŠ¤ë ˆë“œ ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜, \n",
    "        # ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ dz í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©ì„ ìœ ì§€í•¨. (ê¶Œì¥ ì‚¬í•­: ìŠ¤ë ˆë“œ ë‚´ë¶€ì—ì„œ í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±)\n",
    "        asset_resp = dz.get_asset(domainIdentifier=domain_id, identifier=asset_id)\n",
    "        \n",
    "        # 1. GlueTableForm JSON ë¬¸ìì—´ ì¶”ì¶œ ë° íŒŒì‹±\n",
    "        glue_form_content = next((item['content'] for item in asset_resp.get('formsOutput', []) if item['formName'] == 'GlueTableForm'), None)\n",
    "        if not glue_form_content: return None\n",
    "        glue_form_data = json.loads(glue_form_content)\n",
    "        \n",
    "        # 2. ê³ ìœ  í‚¤ ì¶”ì¶œ\n",
    "        db_name = glue_form_data.get(\"databaseIdentifier\") or glue_form_data.get(\"databaseName\")\n",
    "        table_name = glue_form_data.get(\"tableName\")\n",
    "        if not db_name or not table_name: return None\n",
    "        unique_key = f\"{db_name}_{table_name}\"\n",
    "        \n",
    "        # ğŸš€ [ìµœì¢… ìˆ˜ì •] 3. ìµœì¢… ì—…ë°ì´íŠ¸ ì‹œê°„ (Asset Revision ê¸°ë°˜) ì¶”ì¶œ\n",
    "        current_update_time_dt = asset_resp.get(\"createdAt\") \n",
    "        current_time_str = format_time_to_iso(current_update_time_dt)\n",
    "        \n",
    "        # 4. ê²Œì‹œ ìƒíƒœ í™•ì¸\n",
    "        listing_status = (asset_resp.get(\"listing\", {}) or {}).get(\"listingStatus\", \"UNKNOWN\")\n",
    "        \n",
    "        if listing_status == \"ACTIVE\":\n",
    "            asset_info.update({\"unique_key\": unique_key, \"current_update_time\": current_time_str, \"metadata\": asset_resp})\n",
    "            return asset_info\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        if \"ResourceNotFoundException\" in str(e): \n",
    "            # Assetì´ ì™„ì „íˆ ì‚­ì œëœ ê²½ìš°, ì‚­ì œ ê°ì§€ë¥¼ ìœ„í•´ ID ë°˜í™˜\n",
    "            return {\"id\": asset_id, \"status\": \"DELETED\"}\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ğŸ’¡ Step 0: ì´ì „ ìŠ¤ëƒ…ìƒ· ë¡œë“œ ë° ì´ˆê¸°í™”\n",
    "# ----------------------------------------------------------------------\n",
    "previous_update_times = {} \n",
    "if os.path.exists(PUBLISHED_ASSET_SNAPSHOT_FILE):\n",
    "    with open(PUBLISHED_ASSET_SNAPSHOT_FILE, 'r') as f:\n",
    "        previous_update_times = json.load(f)\n",
    "    print(f\"Loaded {len(previous_update_times)} update records for delta comparison.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: Snapshot file not found. Running initial load logic.\", flush=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2ï¸âƒ£ ëª¨ë“  GlueTableAssetTypeì˜ ID ìˆ˜ì§‘ (ì „ì²´ ì¡°íšŒ - ë¸íƒ€ ì²´í¬ë¥¼ ìœ„í•¨)\n",
    "# ----------------------------------------------------------------------\n",
    "projects_resp = dz.list_projects(domainIdentifier=domain_id)\n",
    "projects = {p[\"id\"]: p[\"name\"] for p in projects_resp.get(\"items\", [])}\n",
    "all_glue_asset_ids = []\n",
    "\n",
    "print(\"\\n--- Step 2: Collecting ALL Asset IDs for Delta Check ---\")\n",
    "\n",
    "for project_id, project_name in projects.items():\n",
    "    next_token = None\n",
    "    while True:\n",
    "        params = {\"domainIdentifier\": domain_id, \"owningProjectIdentifier\": project_id, \"searchScope\": \"ASSET\"}\n",
    "        if next_token: params[\"nextToken\"] = next_token\n",
    "\n",
    "        resp = dz.search(**params)\n",
    "        for item in resp.get(\"items\", []):\n",
    "            if item.get(\"assetItem\", {}).get(\"typeIdentifier\") == \"amazon.datazone.GlueTableAssetType\":\n",
    "                all_glue_asset_ids.append({\"id\": item.get(\"assetItem\", {}).get(\"identifier\")})\n",
    "        next_token = resp.get(\"nextToken\")\n",
    "        if not next_token: break\n",
    "\n",
    "print(f\"âœ… Total {len(all_glue_asset_ids)} assets found for detailed check.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3ï¸âƒ£ get_asset í˜¸ì¶œ ë° ë¸íƒ€/ìƒíƒœ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰)\n",
    "# ----------------------------------------------------------------------\n",
    "current_update_snapshot = {}\n",
    "assets_to_save = []\n",
    "MAX_THREADS = 50 \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    results = executor.map(fetch_and_filter_asset, all_glue_asset_ids)\n",
    "    \n",
    "    for asset in results:\n",
    "        if not asset: continue\n",
    "            \n",
    "        unique_key = asset.get(\"unique_key\")\n",
    "        current_time_str = asset.get(\"current_update_time\")\n",
    "        \n",
    "        if not unique_key or not current_time_str: continue \n",
    "            \n",
    "        previous_time_str = previous_update_times.get(unique_key)\n",
    "\n",
    "        # ğŸ’¡ ë¸íƒ€ íŒë‹¨ ë¡œì§ (ì‹œë‚˜ë¦¬ì˜¤ 1: ì¶”ê°€, ì‹œë‚˜ë¦¬ì˜¤ 2: ìˆ˜ì •)\n",
    "        if unique_key not in previous_update_times:\n",
    "            # 1. ì‹ ê·œ ìƒì„± (ì¶”ê°€)\n",
    "            assets_to_save.append({\"asset\": asset, \"action\": \"CREATED\"})\n",
    "        elif previous_time_str and is_time_newer(current_time_str, previous_time_str):\n",
    "            # 2. ìˆ˜ì • (ë®ì–´ì“°ê¸°)\n",
    "            assets_to_save.append({\"asset\": asset, \"action\": \"UPDATED\"})\n",
    "        \n",
    "        # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ í˜„ì¬ ì‹œê°„ ìŠ¤ëƒ…ìƒ· ê¸°ë¡ (ACTIVE ìƒíƒœì¸ ê²½ìš°ë§Œ ê¸°ë¡)\n",
    "        if \"metadata\" in asset:\n",
    "            current_update_snapshot[unique_key] = current_time_str\n",
    "\n",
    "print(f\"\\nâœ… Found {len(assets_to_save)} changes (CREATED/UPDATED) to synchronize.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4ï¸âƒ£ S3/KB í´ë” ë™ê¸°í™” ë¡œì§ (3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬)\n",
    "# ----------------------------------------------------------------------\n",
    "previous_keys = set(previous_update_times.keys())\n",
    "current_keys = set(current_update_snapshot.keys())\n",
    "\n",
    "# ğŸ’¡ 4-A: PUBLISHED í•´ì œ/ì‚­ì œëœ Asset ê°ì§€ ë° ì‚­ì œ (ì‹œë‚˜ë¦¬ì˜¤ 3)\n",
    "keys_to_delete = previous_keys - current_keys\n",
    "\n",
    "for unique_key in keys_to_delete:\n",
    "    output_path = os.path.join(KB_S3_SYNC_DIR, f\"{unique_key}.json\")\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path) # ë¡œì»¬ íŒŒì¼ ì‚­ì œ (S3 API í˜¸ì¶œë¡œ ëŒ€ì²´ í•„ìš”)\n",
    "        print(f\"ğŸ—‘ï¸ Deleted UNPUBLISHED/DELETED Asset: {unique_key}\")\n",
    "\n",
    "# ğŸ’¡ 4-B: ì‹ ê·œ ìƒì„± ë° ë³€ê²½ëœ Asset ì €ì¥ (ì‹œë‚˜ë¦¬ì˜¤ 1 & 2)\n",
    "for item in assets_to_save:\n",
    "    asset = item[\"asset\"]\n",
    "    unique_key = asset[\"unique_key\"]\n",
    "    output_path = os.path.join(KB_S3_SYNC_DIR, f\"{unique_key}.json\") \n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\"unique_key\": unique_key, \"metadata\": asset[\"metadata\"], \"updatedAt\": current_update_snapshot[unique_key]}, \n",
    "            f, default=str, ensure_ascii=False, indent=2\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ… KB File {item['action']}: {unique_key}.json\")\n",
    "\n",
    "# ğŸ’¡ 4-C: í˜„ì¬ ì—…ë°ì´íŠ¸ ì‹œê°„ ìŠ¤ãƒŠãƒƒãƒ—ìƒ· ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•œ ì—…ë°ì´íŠ¸)\n",
    "with open(PUBLISHED_ASSET_SNAPSHOT_FILE, \"w\") as f:\n",
    "    json.dump(current_update_snapshot, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ‰ Daily Delta Asset Management Complete. (Saved {len(assets_to_save)} changes, Deleted {len(keys_to_delete)} keys)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
